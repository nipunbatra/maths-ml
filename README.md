Content



1. Refresher Module

   1. Data representation (floating point arithmetic, underflow, overflow)
   2. Linear Algebra (condition number, matrix decompositions)
   3. Calculus (univariate calculus, chain rule, partial differentiation)
   4. Probability (univariate distributions)
2. Multivariate Calculus and Automatic Differentiation

   1. Gradient, Jacobian, Hessian, and usage in machine learning problems
   2. Finite difference methods and limitations
   3. Computational graph view of differentiation 
   4. Automatic gradient computation (using for example reverse mode auto differentiation)
3. Continuous Multivariate Probability distributions

   1. Visualization and characterisation of commonly used multivariate continuous probability distributions (such as multivariate normal)
   2. Maximum likelihood estimation (MLE) and Maximum A Posteriori (MAP) for estimation of distribution parameters
   3. Change of variables
   4. Conjugate Priors 
4. Optimization

   1. Continuous optimization:

      1. Taylor’s series and connections to optimization methods
      2. Unconstrained Optimization:

         1. First-order optimization:

            1. Gradient descent: step-size, momentum, batching
         2. Second-order optimization:

            1. Gauss-Newton methods
            2. Quasi-Newton methods
      3. Constrained Optimization:

         1. Lagrange multipliers
         2. Primal and Dual Forms
         3. KKT conditions
      4. Linear and Quadratic Programming
5. Unification of Information Theory and Machine Learning

   1. Information Theoretic concepts: self-information, entropy, KL divergence
   2. Perfect communication over a noisy imperfect communication channel: encoder, decoder, error correcting codes (and relation to ensemble methods in machine learning)
   3. Data compression problem: Optimal code, Huffman encoding
6. Markov chains

   1. Sequential modeling examples: energy, temperature, etc.
   2. Limitations of modeling sequential data as independent and identically distributed (IID)
   3. Markov Chain characterisation (prior, transition)
   4. Joint probability of a sequence and first and Kth order Markov chain. 
   5. Markov chain representation (finite state machine, adjacency matrix, flat tabular)
   6. Probability of a sequence given Markov chain parameters
   7. Learning the parameters (prior and transition matrix) for a Markov chain
